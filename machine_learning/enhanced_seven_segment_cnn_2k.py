#!/usr/bin/env python3
"""
Enhanced Seven-Segment CNN Fine-tuning for 2K Dataset
=====================================================

This script implements an advanced CNN for seven-segment digit recognition
using a much larger 2K dataset (2000 images, 200 per digit).

Key improvements:
- Optimized for larger dataset
- Advanced data augmentation
- Improved model architecture
- Better training strategy
- Early stopping and learning rate scheduling
"""

import os
import sys
import json
import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import MobileNetV2
import seaborn as sns
from tqdm import tqdm
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SevenSegmentCNN2K:
    """
    Enhanced CNN for seven-segment digit recognition with 2K dataset
    """
    
    def __init__(self, input_shape=(224, 224, 3), num_classes=10):
        """
        Initialize the CNN model
        
        Args:
            input_shape: Input image shape
            num_classes: Number of output classes (digits 0-9)
        """
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = None
        self.history = None
        
    def load_dataset_2k(self, dataset_path: str = "seven_segment_dataset_2k/images"):
        """
        Load the 2K seven-segment dataset
        
        Args:
            dataset_path: Path to dataset images
            
        Returns:
            Tuple of (images, labels)
        """
        print("Loading 2K dataset...")
        
        dataset_dir = Path(dataset_path)
        if not dataset_dir.exists():
            raise ValueError(f"Dataset directory not found: {dataset_path}")
        
        images = []
        labels = []
        
        # Load all images
        image_files = list(dataset_dir.glob("*.png"))
        
        for img_path in tqdm(image_files, desc="Loading images"):
            try:
                # Extract digit label from filename (e.g., seven_segment_0_0000.png -> 0)
                digit = int(img_path.stem.split('_')[2])
                
                # Load and preprocess image
                img = cv2.imread(str(img_path))
                if img is None:
                    continue
                    
                # Convert BGR to RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                
                # Resize to model input size
                img = cv2.resize(img, (self.input_shape[1], self.input_shape[0]))
                
                images.append(img)
                labels.append(digit)
                
            except (ValueError, IndexError) as e:
                logger.warning(f"Skipping invalid file {img_path}: {e}")
                continue
        
        images = np.array(images, dtype=np.float32) / 255.0
        labels = np.array(labels)
        
        print(f"Loaded {len(images)} images with shape {images.shape}")
        
        # Print distribution
        unique, counts = np.unique(labels, return_counts=True)
        print(f"Label distribution: {dict(zip(unique, counts))}")
        
        return images, labels
    
    def create_advanced_augmentation(self):
        """
        Create advanced data augmentation pipeline optimized for 2K dataset
        
        Returns:
            Data augmentation function
        """
        def augment_image(image, label):
            """Apply augmentation to a single image"""
            
            # Random brightness (milder range for seven-segment displays)
            image = tf.image.random_brightness(image, 0.15)
            
            # Random contrast (seven-segment displays benefit from contrast)
            image = tf.image.random_contrast(image, 0.85, 1.15)
            
            # Random saturation (subtle for seven-segment displays)
            image = tf.image.random_saturation(image, 0.9, 1.1)
            
            # Random horizontal flip (50% chance) - be careful with digits
            # Only apply to symmetric digits or disable for asymmetric ones
            if tf.random.uniform(()) > 0.8:  # Reduced probability
                image = tf.image.random_flip_left_right(image)
            
            # Random rotation (small angles only for seven-segment)
            if tf.random.uniform(()) > 0.7:
                angle = tf.random.uniform((), -5, 5) * np.pi / 180  # Â±5 degrees only
                # Note: Simplified rotation - for production use tf.image.rotate
                
            # Random noise (very subtle for digital displays)
            if tf.random.uniform(()) > 0.8:
                noise = tf.random.normal(tf.shape(image), stddev=0.01)  # Reduced noise
                image = tf.clip_by_value(image + noise, 0.0, 1.0)
            
            # Random gamma correction (simulates lighting changes)
            if tf.random.uniform(()) > 0.7:
                gamma = tf.random.uniform((), 0.8, 1.2)
                image = tf.image.adjust_gamma(image, gamma)
            
            # Ensure image values are in [0, 1]
            image = tf.clip_by_value(image, 0.0, 1.0)
            
            return image, label
        
        return augment_image
    
    def build_enhanced_model(self):
        """
        Build enhanced CNN model optimized for larger dataset
        
        Returns:
            Compiled Keras model
        """
        print("Building enhanced CNN model for 2K dataset...")
        
        # Create base model with MobileNetV2
        base_model = MobileNetV2(
            input_shape=self.input_shape,
            alpha=1.0,  # Full model for better capacity
            include_top=False,
            weights='imagenet'
        )
        
        # Freeze only the first 100 layers (instead of all) for better learning
        base_model.trainable = False
        
        # Build model with improved architecture
        model = keras.Sequential([
            # Input normalization (better range for MobileNetV2)
            layers.Rescaling(1./127.5, offset=-1),
            
            # Base model
            base_model,
            
            # Global pooling with spatial information retention
            layers.GlobalAveragePooling2D(),
            
            # Regularization
            layers.Dropout(0.2),  # Reduced dropout for better learning
            
            # Dense layers with better capacity
            layers.Dense(512, activation='relu'),  # Increased capacity
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.2),
            
            # Output layer with better initialization
            layers.Dense(self.num_classes, activation='softmax',
                        kernel_initializer='he_normal')
        ])
        
        # Compile model with optimized settings
        model.compile(
            optimizer=Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        self.model = model
        
        # Build the model with dummy input to get correct parameter counts
        dummy_input = tf.zeros((1,) + self.input_shape)
        model(dummy_input)
        
        # Print model summary
        model.summary()
        print(f"Total params: {model.count_params():,}")
        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
        print(f"Trainable params: {trainable_params:,}")
        
        return model
    
    def train_enhanced_model(self, X_train, y_train, X_val, y_val, 
                           epochs_phase1=12, epochs_phase2=8, batch_size=32):
        """
        Train the enhanced model with two-phase approach
        
        Args:
            X_train, y_train: Training data
            X_val, y_val: Validation data
            epochs_phase1: Epochs for frozen base training (reduced for faster convergence)
            epochs_phase2: Epochs for fine-tuning
            batch_size: Batch size (reduced for better stability)
        """
        print("\n" + "="*60)
        print("ENHANCED TWO-PHASE TRAINING FOR 2K DATASET")
        print("="*60)
        
        # Prepare data augmentation
        augment_fn = self.create_advanced_augmentation()
        
        # Create augmented training dataset
        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
        train_ds = train_ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)
        train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
        
        # Validation dataset (no augmentation)
        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))
        val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
        
        # Callbacks with better settings
        callbacks = [
            ModelCheckpoint(
                'seven_segment_2k_enhanced_model.keras',
                monitor='val_accuracy',
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            EarlyStopping(
                monitor='val_accuracy',
                patience=5,  # Reduced patience for faster training
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,  # Reduced patience
                min_lr=1e-7,
                verbose=1
            ),
            TensorBoard(
                log_dir='logs/seven_segment_2k',
                histogram_freq=1
            )
        ]
        
        # Phase 1: Train with frozen base model
        print(f"\nPhase 1: Training with frozen base model ({epochs_phase1} epochs)...")
        print(f"Learning rate: {self.model.optimizer.learning_rate.numpy()}")
        
        history1 = self.model.fit(
            train_ds,
            epochs=epochs_phase1,
            validation_data=val_ds,
            callbacks=callbacks,
            verbose=1
        )
        
        # Phase 2: Fine-tune with unfrozen layers
        print(f"\nPhase 2: Fine-tuning with unfrozen layers ({epochs_phase2} epochs)...")
        
        # Unfreeze only the last few layers for gradual fine-tuning
        self.model.layers[1].trainable = True
        
        # Freeze the first 100 layers to prevent overfitting
        for layer in self.model.layers[1].layers[:100]:
            layer.trainable = False
        
        # Use lower learning rate for fine-tuning
        self.model.compile(
            optimizer=Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"Learning rate: {self.model.optimizer.learning_rate.numpy()}")
        print(f"Trainable params after unfreezing: {sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights]):,}")
        
        # Continue training
        history2 = self.model.fit(
            train_ds,
            epochs=epochs_phase2,
            validation_data=val_ds,
            callbacks=callbacks,
            verbose=1,
            initial_epoch=len(history1.history['loss'])
        )
        
        # Combine histories safely
        self.history = {}
        for key in history1.history.keys():
            if key in history2.history:
                self.history[key] = history1.history[key] + history2.history[key]
            else:
                # If key doesn't exist in phase 2, just use phase 1
                self.history[key] = history1.history[key]
        
        # Add any keys that only exist in phase 2
        for key in history2.history.keys():
            if key not in self.history:
                # Pad with zeros for phase 1 length
                phase1_length = len(history1.history['loss'])
                padded_values = [0.0] * phase1_length
                self.history[key] = padded_values + history2.history[key]
        
        print("\nTraining completed!")
        return self.history
    
    def evaluate_model(self, X_test, y_test):
        """
        Evaluate the trained model
        
        Args:
            X_test, y_test: Test data
            
        Returns:
            Dictionary with evaluation results
        """
        print("\nEvaluating enhanced 2K model...")
        
        # Get predictions
        y_pred_proba = self.model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # Calculate metrics
        test_loss, test_accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        
        # Classification report
        report = classification_report(y_test, y_pred, output_dict=True)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        
        # Per-class accuracy
        class_accuracy = {}
        for i in range(10):
            mask = y_test == i
            if np.sum(mask) > 0:
                class_accuracy[f'digit_{i}'] = np.mean(y_pred[mask] == i)
        
        results = {
            'test_loss': test_loss,
            'test_accuracy': test_accuracy,
            'classification_report': report,
            'confusion_matrix': cm.tolist(),
            'class_accuracy': class_accuracy
        }
        
        print(f"Test Accuracy: {test_accuracy:.4f}")
        print(f"Test Loss: {test_loss:.4f}")
        
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        print("\nAccuracy by digit:")
        for digit, acc in class_accuracy.items():
            print(f"{digit}: {acc:.3f}")
        
        return results
    
    def plot_training_history(self):
        """Plot training history"""
        if self.history is None:
            print("No training history available")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Plot accuracy
        ax1.plot(self.history['accuracy'], label='Training Accuracy')
        ax1.plot(self.history['val_accuracy'], label='Validation Accuracy')
        ax1.set_title('Model Accuracy (2K Dataset)')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Accuracy')
        ax1.legend()
        ax1.grid(True)
        
        # Plot loss
        ax2.plot(self.history['loss'], label='Training Loss')
        ax2.plot(self.history['val_loss'], label='Validation Loss')
        ax2.set_title('Model Loss (2K Dataset)')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history_2k.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_model_and_results(self, results, filename_base="seven_segment_2k"):
        """
        Save model and training results
        
        Args:
            results: Evaluation results
            filename_base: Base filename for saved files
        """
        # Save final model
        final_model_path = f"{filename_base}_final.keras"
        self.model.save(final_model_path)
        print(f"Model saved as: {final_model_path}")
        
        # Save results
        results_path = f"{filename_base}_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"Results saved as: {results_path}")
        
        # Save training history
        if self.history:
            history_path = f"{filename_base}_history.json"
            with open(history_path, 'w') as f:
                json.dump(self.history, f, indent=2)
            print(f"Training history saved as: {history_path}")


def main():
    """Main training function"""
    print("Enhanced Seven-Segment CNN Fine-tuning with 2K Dataset")
    print("=" * 60)
    
    # Initialize model
    cnn = SevenSegmentCNN2K()
    
    # Load 2K dataset
    try:
        X, y = cnn.load_dataset_2k()
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return
    
    # Split dataset (70% train, 20% val, 10% test)
    print("\nSplitting dataset...")
    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X, y, test_size=0.1, random_state=42, stratify=y
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val, test_size=0.22, random_state=42, stratify=y_train_val
    )
    
    print(f"Train set: {len(X_train)} samples")
    print(f"Validation set: {len(X_val)} samples") 
    print(f"Test set: {len(X_test)} samples")
    
    # Build and compile model
    cnn.build_enhanced_model()
    
    # Train model with optimized parameters
    history = cnn.train_enhanced_model(
        X_train, y_train, X_val, y_val,
        epochs_phase1=10,  # Reduced for faster training and better convergence
        epochs_phase2=6,   # Reduced for stability
        batch_size=32      # Reduced for better stability with larger dataset
    )
    
    # Evaluate model
    results = cnn.evaluate_model(X_test, y_test)
    
    # Plot training history
    cnn.plot_training_history()
    
    # Save everything
    cnn.save_model_and_results(results)
    
    # Performance summary
    print("\n" + "="*60)
    print("ENHANCED 2K DATASET TRAINING COMPLETED")
    print("="*60)
    print(f"Final Test Accuracy: {results['test_accuracy']:.1%}")
    print(f"Improvement over original: {results['test_accuracy']/0.50:.1f}x")
    
    if results['test_accuracy'] > 0.85:
        print("ðŸŽ‰ EXCELLENT: Model achieved >85% accuracy!")
    elif results['test_accuracy'] > 0.75:
        print("âœ… GOOD: Model achieved >75% accuracy!")
    elif results['test_accuracy'] > 0.65:
        print("ðŸ‘ DECENT: Model achieved >65% accuracy!")
    else:
        print("âš  Model needs further optimization")
    
    print(f"Enhanced model saved as: seven_segment_2k_final.keras")
    print("Ready for production integration!")


if __name__ == "__main__":
    main()
